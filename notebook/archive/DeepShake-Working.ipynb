{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lucky-paraguay",
   "metadata": {},
   "source": [
    "# DeepShake Initial Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "line_length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-eagle",
   "metadata": {},
   "source": [
    "## Prelim: Simple finetune on small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are just going to do sequence to sequence as a first cut: repeatedly pick a line of a sonnet at random, \n",
    "# generate some noise, and train the model to turn the noise into the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pass: use translator pipeline with t5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"translate English to French: Shall I compare thee to a summer's day\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(line_length, (1, 1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-destiny",
   "metadata": {},
   "source": [
    "## Load lines as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poetry_into_lines(filename):\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lines = [line.strip().split(' ') for line in text.split('\\n') if len(line.strip()) > 0 and not (line.isupper())]\n",
    "#     lines = [line.]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_feed_forward(line_number):\n",
    "    input_first = tokenizer.encode(\"translate English to French: \" + \" \".join(shake_sonnet_lines[line_number]), return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_first, max_length=20, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_sonnet_lines = read_poetry_into_lines('../data/shakespeare-sonnets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_sonnet_lines[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_feed_forward(105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = [f\"translate English to French: {' '.join(line)}\" for line in shake_sonnet_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded = tokenizer.batch_encode_plus(all_inputs[:10], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(all_encoded.input_ids, max_length = 30, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-shape",
   "metadata": {},
   "source": [
    "### Set up \"translator\" training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-psychology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-answer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "social-programming",
   "metadata": {},
   "source": [
    "## Language GAN approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-clark",
   "metadata": {},
   "source": [
    "Core approach: LanguageGAN, borrowing some ideas from ColdGAN and CTRLModel, in time using either an input class or a cycle loss to train it close to a certain style. Elements:\n",
    "\n",
    "* _Generator_: Noise based transformer with max sequence length 10 (= 10 words, max length of a sonnet line, if all one syllable). Trained using REINFOCE on implicit policy to generate next token.\n",
    "* _Discriminator_: Transformer architecture as well. Trained using normal way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size=5092, \n",
    "                projection_size=512, \n",
    "                batch_size=512, \n",
    "                input_dropout=0.1, \n",
    "                output_dropout=0.1,\n",
    "                repetition_penalty=1.2,\n",
    "                temperature=0.3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.projection_size=512\n",
    "        self.batch_size = batch_size\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.input_proj = nn.Linear(self.vocab_size, self.projection_size)\n",
    "        \n",
    "        self.\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.input_proj.weight.uniform_(-initrange, initrange)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        print('Fake!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-serum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepshake",
   "language": "python",
   "name": "deepshake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
