{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import subprocess \n",
    "import gc\n",
    "import csv\n",
    "import os.path\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic utilities for GPU mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plundered from: https://github.com/huggingface/transformers/issues/1742\n",
    "def show_gpu(msg):\n",
    "    \"\"\"\n",
    "    ref: https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\n",
    "    \"\"\"\n",
    "    def query(field):\n",
    "        return(subprocess.check_output(\n",
    "            ['nvidia-smi', f'--query-gpu={field}',\n",
    "                '--format=csv,nounits,noheader'], \n",
    "            encoding='utf-8'))\n",
    "    def to_int(result):\n",
    "        return int(result.strip().split('\\n')[0])\n",
    "    \n",
    "    used = to_int(query('memory.used'))\n",
    "    total = to_int(query('memory.total'))\n",
    "    pct = used/total\n",
    "    print('\\n' + msg, f'{100*pct:2.1f}% ({used} out of {total})')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_gpu():\n",
    "    show_gpu('Preclean: ')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    show_gpu('Postclean: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(model):\n",
    "    del model\n",
    "    clean_up_gpu()\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "show_gpu('Initial use:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poetry_into_lines(filename):\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    lines = [line.strip().split(' ') for line in text.split('\\n') if len(line.strip()) > 0 and not (line.isupper())]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_sonnet_lines = read_poetry_into_lines('./data/shakespeare-sonnets.txt')\n",
    "print('Number of lines: ', len(shake_sonnet_lines))\n",
    "max_line_length = max([len(line) for line in shake_sonnet_lines])\n",
    "print('Max length of line: ', max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_once_off(model, tokenizer, skip_special_tokens=False):\n",
    "    random_words = random.sample(list(tokenizer.vocab.keys()), 3)\n",
    "    tokenized_random = tokenizer(f\"generate Shakespeare: {' '.join(random_words)}\", padding=True, return_tensors=\"pt\").to(device)\n",
    "    model_output = model.generate(**tokenized_random, max_length=max_line_length)\n",
    "    print('Random words: ', random_words, ' and output: ', tokenizer.decode(model_output[0], skip_special_tokens=skip_special_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_paired_noise_lines(tokenizer, command=\"shakespeare\", number_pairs=1, input_length=5, add_eos_token=False):\n",
    "    generated_noise = []\n",
    "    actual_lines = []\n",
    "    max_sample = len(shake_sonnet_lines)\n",
    "    number_loops = ceil(number_pairs / max_sample)\n",
    "    for i in range(0, number_loops):\n",
    "        loop_pairs = min(max_sample, number_pairs - len(actual_lines))\n",
    "        random_set = [random.sample(list(tokenizer.vocab.keys()), input_length) for _ in range(loop_pairs)] \n",
    "        generated_noise = generated_noise + [f\"{command}: {' '.join(random_words)}\" for random_words in random_set]\n",
    "        lines_in_sample = random.sample(shake_sonnet_lines, loop_pairs)\n",
    "        if add_eos_token:\n",
    "            lines_in_sample = [line + [tokenizer.eos_token] for line in lines_in_sample]\n",
    "        actual_lines = actual_lines + [' '.join(line) for line in lines_in_sample]\n",
    "    return generated_noise, actual_lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseToShakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, generated_noise, paired_lines):\n",
    "        self.generated_noise = generated_noise\n",
    "        self.sonnet_lines = paired_lines\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return { 'tgt_texts': self.sonnet_lines[idx], 'src_texts': self.generated_noise[idx], 'id': idx }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.generated_noise)\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batch_encoding = tokenizer.prepare_seq2seq_batch(\n",
    "            [x[\"src_texts\"] for x in batch],\n",
    "            tgt_texts=[x[\"tg_texts\"] for x in batch],\n",
    "            return_tensors=\"pt\"\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(number_pairs, tokenizer):\n",
    "    file_path = f\"./data/paired_noise_lines_{tokenizer.name_or_path}_{number_pairs}.csv\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        return False, None, None\n",
    "    \n",
    "    with open(file_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None)\n",
    "        combined = list(reader)\n",
    "        inputs, labels = map(list, zip(*combined))\n",
    "                \n",
    "    return True, inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(number_pairs, tokenizer, inputs, labels):\n",
    "    input_path = f\"./data/paired_noise_lines_{tokenizer.name_or_path}_{number_pairs}.csv\"\n",
    "        \n",
    "    with open(input_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['input', 'label'])\n",
    "        writer.writerows(list(zip(inputs, labels)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(tokenizer, number_pairs, check_for_saved=False, write_generated=False, tokenizer_name=None):\n",
    "    loaded_prior = False\n",
    "    if check_for_saved:\n",
    "        loaded_prior, inputs, labels = read_dataset(number_pairs, tokenizer)\n",
    "        \n",
    "    if not loaded_prior:\n",
    "        inputs, labels = assemble_paired_noise_lines(tokenizer, number_pairs=number_pairs, add_eos_token=True)\n",
    "    \n",
    "    if write_generated and not loaded_prior:\n",
    "        write_dataset(number_pairs, tokenizer, inputs, labels)\n",
    "        \n",
    "    return NoiseToShakeDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseToShakeDataCollator:\n",
    "    def __init__(self, tokenizer, data_args=None):\n",
    "        self.data_args = data_args\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = self._encode(batch)\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    "        )\n",
    "        \n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "#             \"decoder_input_ids\": labels,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def _encode(self, batch):\n",
    "#         print(batch)\n",
    "        batch_encoding = self.tokenizer.prepare_seq2seq_batch(\n",
    "            [x[\"src_texts\"] for x in batch],\n",
    "            tgt_texts=[x[\"tgt_texts\"] for x in batch],\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_line_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintExampleCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
    "        output_once_off(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model_base, \n",
    "    tokenizer_base, \n",
    "    pretrained_name,\n",
    "    training_args,\n",
    "    number_training_pairs=2000,\n",
    "    number_validation_pairs=100,\n",
    "    add_eos_token_to_labels=False, \n",
    "    verbose=False\n",
    "):\n",
    "    model = model_base.from_pretrained(\"t5-base\").to(device)\n",
    "    tokenizer = tokenizer_base.from_pretrained(\"t5-base\")\n",
    "    clean_up_gpu()\n",
    "    show_gpu('Loaded model, current GP use:')\n",
    "    \n",
    "    if verbose:\n",
    "        output_once_off(model, tokenizer)\n",
    "        noise, line = assemble_paired_noise_lines(tokenizer, add_eos_token=add_eos_token_to_labels)\n",
    "        print(noise, line)\n",
    "        print(NoiseToShakeDataset(noise, line)[0])\n",
    "    \n",
    "    train_dataset = assemble_dataset(tokenizer, number_pairs=number_training_pairs, check_for_saved=True, write_generated=True)\n",
    "    val_dataset = assemble_dataset(tokenizer, number_pairs=number_validation_pairs, check_for_saved=True, write_generated=True)\n",
    "    \n",
    "    clean_up_gpu()\n",
    "    show_gpu('After composing datasets: ')\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=NoiseToShakeDataCollator(tokenizer=tokenizer),\n",
    "        callbacks=[PrintExampleCallback()]\n",
    "    )\n",
    "        \n",
    "    trainer.train()\n",
    "    \n",
    "    if verbose:\n",
    "        output_once_off(model, tokenizer)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=128,  # batch size per device during training\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    adafactor=True,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = run_experiment(\n",
    "    model_base=AutoModelForSeq2SeqLM, \n",
    "    tokenizer_base=AutoTokenizer, \n",
    "    pretrained_name=\"t5-base\", \n",
    "    training_args=training_args, \n",
    "    number_training_pairs=20000,\n",
    "    add_eos_token_to_labels=True, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m61"
  },
  "kernelspec": {
   "display_name": "deepshake",
   "language": "python",
   "name": "deepshake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
