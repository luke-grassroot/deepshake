{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepshake import experiment, generate_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_utils import show_gpu, read_poetry_into_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {}\n",
    "data_files[\"train\"] = \"./data/shakespeare_sonnets_train.json\"\n",
    "data_files[\"validation\"] = \"./data/shakespeare_sonnets_eval.json\"\n",
    "datasets = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn tiny shakespeare into what we want\n",
    "\n",
    "create_data = False\n",
    "\n",
    "if create_data:\n",
    "    shakeds = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
    "    lines = shakeds[0]['text'].split('\\n')\n",
    "    character = re.compile(\"^[a-zA-Z]*:$\")\n",
    "    actual_lines = [line for line in lines if not bool(character.match(line)) and len(line.strip()) > 0 and len(line.split()) > 3]\n",
    "    lines_to_write = [{\"text\": line} for line in actual_lines]\n",
    "    \n",
    "    with open(\"./data/shakespeare_lines_train.json\", \"w\") as fout:\n",
    "        [fout.write(json.dumps(line) + \"\\n\") for line in lines_to_write[:-1000]]\n",
    "\n",
    "    with open(\"./data/shakespeare_lines_eval.json\", \"w\") as fout:\n",
    "        [fout.write(json.dumps(line) + \"\\n\") for line in lines_to_write[-1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "show_gpu('Initial use:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-97bbec7a35cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gpt_long_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt_long_train\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: possible pretrained variants with AutoModelForSeq2Seq:\n",
    "* T5\n",
    "* MT5\n",
    "* Bart (incl MBart\n",
    "* LEDConfig\n",
    "* Blenderbot\n",
    "* Pegasus\n",
    "* Marian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load_model(AutoModelForSeq2SeqLM, AutoTokenizer, 't5_base_20k_1line', backup_tokenizer_name='t5-base', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token_id=GPT2Tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpt_output(output_tensor):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(tokenizer.decode(output_tensor[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shake_sonnet_lines = read_poetry_into_lines('./data/shakespeare-sonnets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shake_sonnet_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode('Shakespeare: ', return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpt_output(greedy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpt_output(model.generate(input_ids, max_length=140, num_beams=5, early_stopping=True, no_repeat_ngram_size=2))\n",
    "# print_gpt_output(model.generate(input_ids, max_length=140, num_beams=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpt_output(model.generate(input_ids, do_sample=True, max_length=140, top_k=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpt_output(model.generate(input_ids, do_sample=True, max_length=50, top_p=0.92, top_k=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"If thou wilt thou art belovd of many<newline>Then why dost thou use words thine own<newline>And give invention a try<newline>While I in thy verses do compile<newline>That Tome of thyself doth publish every where<newline>Thou art streaking with untouchd shame<newline>And yet love knows it is a greater grief<newline>To bear loves wrong than hates known injury<newline>Lascivious grace in whom all ill well shows<newline>Kill me with spites yet we must not be foesThose pretty wrongs that liberty commits\"\n",
    "as_poem = output.split(\"<newline>\")\n",
    "print(\"\\n\".join(as_poem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attempted_sonnet(init_word=\"Shakespeare: \", max_length=140):\n",
    "    input_ids = tokenizer.encode(init_word, return_tensors='pt').to(device)\n",
    "    \n",
    "    # human opinion indicates these args to generate are yielding best results (will do more systematic search next)\n",
    "    coin_toss = random.random()\n",
    "    if coin_toss > 0.5:\n",
    "        output_tensor = model.generate(input_ids, max_length=140, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)\n",
    "    else:\n",
    "        output_tensor = model.generate(input_ids, do_sample=True, max_length=140, top_k=50)\n",
    "    \n",
    "    output_result = tokenizer.decode(output_tensor[0], skip_special_tokens=True)\n",
    "    as_poem = output_result.split(\"<newline>\")\n",
    "    print(\"\\n\".join(as_poem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_attempted_sonnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen_args = {\"temperature\": \"2\", \"min_length\": 20, \"repetition_penalty\": 5. }\n",
    "\n",
    "custom_training_args = {\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"adafactor\": True,\n",
    "    \"learning_rate\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pair_sizes = [20000, 40000, 80000, 200000]\n",
    "lines_per_pair = [1, 2, 4, 8]\n",
    "input_lengths = [5, 10, 20, 40]\n",
    "\n",
    "name_base=\"experiment_run_210307\"\n",
    "run_experiments=False\n",
    "\n",
    "if run_experiments:\n",
    "    for pair_size in training_pair_sizes:\n",
    "        for lines_pp in lines_per_pair:\n",
    "            for i_length in input_lengths:\n",
    "                experiment_name = f\"{name_base}_{i_length}_{lines_pp}_{pair_size}\"\n",
    "    #             print(experiment_name)\n",
    "                sonnets = experiment(\n",
    "                    label=experiment_name,\n",
    "                    model_base=AutoModelForSeq2SeqLM, \n",
    "                    tokenizer_base=AutoTokenizer, \n",
    "                    pretrained_name=\"t5-base\", \n",
    "                    custom_training_args=custom_training_args,\n",
    "                    lines_per_pair=lines_pp,\n",
    "                    number_training_pairs=pair_size,\n",
    "                    number_validation_pairs=1000,\n",
    "                    input_length=i_length,\n",
    "                    add_eos_token_to_labels=True, \n",
    "                    verbose=False\n",
    "                )\n",
    "                print(experiment_name, \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_sonnet_lines = read_poetry_into_lines('./data/shakespeare-sonnets.txt')\n",
    "shake_sonnet_matchers = [SequenceMatcher(None, '', shake_line) for shake_line in shake_sonnet_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_seq_match(line):\n",
    "    ratios = []\n",
    "    for matcher in shake_sonnet_matchers:\n",
    "        matcher.set_seq1(line)\n",
    "        ratios.append(matcher.ratio())\n",
    "    return max(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = read_poetry_into_lines('./results/written_lines/experiment_run_210307_20_2_200000')\n",
    "is_poem_title = lambda line: len(line) == 1 and line[0].isnumeric()\n",
    "experiment_results = [line for line in experiment_results if not is_poem_title(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_plagiarized(line, threshold=0.8): # eyeballing, this seems about where it gets so similar as to be a repeat\n",
    "    # simple for now (some lines are plagiarized on phrases)\n",
    "    seq_matches = get_max_seq_match(line)\n",
    "    return seq_matches > threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = 'And yet methinks I had astronomy'.split(' ')\n",
    "# sh_line_index = shake_sonnet_lines.index(test_line)\n",
    "orig_line = shake_sonnet_lines[183]\n",
    "print(is_plagiarized(test_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_and = len([line for line in experiment_results if line[0] == 'And'])\n",
    "number_plagiarized = len([line for line in experiment_results if is_plagiarized(line)])\n",
    "print('Number of lines generated: ', len(experiment_results))\n",
    "print('Number start with and: ', number_and)\n",
    "print('Number plagiarized: ', number_plagiarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_plus_plague(experiment_base, input_length, lines_per_pair, number_pairs):\n",
    "    file_name = f\"./results/written_lines/{experiment_base}_{input_length}_{lines_per_pair}_{number_pairs}\"\n",
    "    if not isfile(file_name):\n",
    "        return 0, 0, 0\n",
    "    experiment_results = read_poetry_into_lines(file_name)\n",
    "    is_poem_title = lambda line: len(line) == 1 and line[0].isnumeric()\n",
    "    experiment_results = [line for line in experiment_results if not is_poem_title(line)]\n",
    "    number_and = len([line for line in experiment_results if line[0] == 'And'])\n",
    "    number_plagiarized = len([line for line in experiment_results if is_plagiarized(line)])\n",
    "    return len(experiment_results), number_and, number_plagiarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = []\n",
    "name_base=\"experiment_run_210307\"\n",
    "# compute_and_plus_plague(name_base, 20, 2, 200000)\n",
    "\n",
    "for pair_size in training_pair_sizes:\n",
    "    for lines_pp in lines_per_pair:\n",
    "        for i_length in input_lengths:\n",
    "            number_generated, number_start_with_and, number_plagiarized = compute_and_plus_plague(name_base, i_length, lines_pp, pair_size)\n",
    "            experiment_results += [{\n",
    "                'training_size': pair_size,\n",
    "                'lines_per_pair': lines_pp,\n",
    "                'input_length': i_length,\n",
    "                'number_lines_gen': number_generated,\n",
    "                'number_with_and': number_start_with_and,\n",
    "                'number_plagiarized': number_plagiarized\n",
    "            }]\n",
    "            print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_df = pd.DataFrame(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_df = er_df[er_df.number_lines_gen > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(er_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_df.to_csv('results/experiment_batch_210307')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m61"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}